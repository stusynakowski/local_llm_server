[Unit]
Description=vLLM Server
After=network.target

[Service]
Type=simple
User=stu
WorkingDirectory=/home/stu/Github/local_llm_server
Environment="PATH=/home/stu/Github/local_llm_server/venv/bin:/usr/local/bin:/usr/bin:/bin"
ExecStart=/home/stu/Github/local_llm_server/venv/bin/python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.3-70B-Instruct \
    --quantization awq \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.95 \
    --max-model-len 4096
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
