# Server Configuration
HOST=0.0.0.0
OLLAMA_PORT=11434
VLLM_PORT=8000

# Model Configuration
DEFAULT_MODEL=llama3.3:70b-instruct-q4_K_M
VLLM_MODEL=meta-llama/Llama-3.3-70B-Instruct

# GPU Settings
CUDA_VISIBLE_DEVICES=0
GPU_MEMORY_UTILIZATION=0.95

# vLLM Settings
VLLM_QUANTIZATION=awq
MAX_MODEL_LEN=4096
TENSOR_PARALLEL_SIZE=1
